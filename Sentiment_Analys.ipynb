{
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Sentiment Analysis | Simple RNN vs LSTM ü¶æüìù",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 320111,
          "sourceType": "datasetVersion",
          "datasetId": 134715
        }
      ],
      "dockerImageVersionId": 30646,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shabbarsyed/Cash-Ratio-Analysis/blob/main/Sentiment_Analys.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'imdb-dataset-of-50k-movie-reviews:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F134715%2F320111%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240219%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240219T215644Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8d8530c90d99037f864df6a7bfd8785c987645455370e167151f1a66695becdf4ec16d37c163098ff6e303372b2d812d351f010e88e4c22a2c614143992ac4fe58b6a851136fce22e93fb99f31ce6872af5c4702660879c0bb3d3e74059e126ee7ab96781a162a814f3d092917f5f4b81cc6be3185c79fe15688bd11bacf99c41848a0c38c0c60f2bf46ecc934821fd8c349ec0f406745d9e5da95adc66d105833aab3d75e6d1628362a95e79adc536aae8bdf45993be20490ee824c56ec712766b8de402f8c6459a095f9c26a319e2231c17c6b0892e1ddb66497b6d789fb7ef3a774708cb34c8d24ba15812aebde217f03621092c812b19d88a90a307caa57'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "oQQhzWzpXzWs"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dependencies"
      ],
      "metadata": {
        "id": "Zqr-sfCYz4hF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from numpy import array\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.text import one_hot, Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Dense\n",
        "from tensorflow.keras.layers import LSTM, SimpleRNN, Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "64u94UgDbV66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data"
      ],
      "metadata": {
        "id": "gvQVViLT0HuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('IMDB Dataset.csv')"
      ],
      "metadata": {
        "id": "QMdRZZjsfYfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "DfmjkLKrezrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "MFfMih_JgQGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing"
      ],
      "metadata": {
        "id": "6Z8OFdOQ0bOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().any()"
      ],
      "metadata": {
        "id": "GrSdT237gSx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['review'][4]"
      ],
      "metadata": {
        "id": "jeMxCPwYgS0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Removing '<--->' Tag*\n",
        "### Using Regular Expression we remove (<--><-->) type tags"
      ],
      "metadata": {
        "id": "70wMMlLk0iND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " Tag = re.compile(r'<[^>]+>')\n",
        "\n",
        " def remove(text):\n",
        "  return Tag.sub('', text)"
      ],
      "metadata": {
        "id": "ZB9Oh_S3h2Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new='''Petter Mattei\\'s \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler\\'s play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei\\'s direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work'''\n",
        "\n",
        "remove(new)"
      ],
      "metadata": {
        "id": "UvZKXjpTiYXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Filtering important words*\n",
        "### Removing unimportant characters, numbers, spaces... etc using Regular Expression"
      ],
      "metadata": {
        "id": "W5H1ztnU1CXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "hp8kw-meitQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sen):\n",
        "     sentence = sen.lower()\n",
        "\n",
        "     # Remove html tags\n",
        "     sentence = remove(sentence)\n",
        "\n",
        "     # Remove punctuations and numbers\n",
        "     sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "     # Single character removal\n",
        "     sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "     # Remove multiple spaces\n",
        "     sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "     # Remove Stopwords\n",
        "     pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
        "     sentence = pattern.sub('', sentence)\n",
        "\n",
        "     return sentence"
      ],
      "metadata": {
        "id": "j25IekSDi1i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(new)"
      ],
      "metadata": {
        "id": "E5mwq_o4jv1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Trainable Data"
      ],
      "metadata": {
        "id": "BnnBMtUA1fkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "sentences = list(data['review'])\n",
        "for sen in sentences:\n",
        "    X.append(preprocess(sen))"
      ],
      "metadata": {
        "id": "iec5L8aQj2OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[4]"
      ],
      "metadata": {
        "id": "7qiSHGG5lIBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = data['sentiment']\n",
        "\n",
        "Y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, Y)))"
      ],
      "metadata": {
        "id": "BHDcI8X3lMUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Train Test Split"
      ],
      "metadata": {
        "id": "SVtERZrj1lpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "oAilOrLKm4-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing Words into numerical sequences"
      ],
      "metadata": {
        "id": "IQnFeNt120BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
        "X_test = word_tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "YXCpihpVlMXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_length = len(word_tokenizer.word_index)+1\n",
        "vocab_length"
      ],
      "metadata": {
        "id": "-aei1WJHlIE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding"
      ],
      "metadata": {
        "id": "8rP7jt_M295f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=100\n",
        "X_train=pad_sequences(X_train, padding='post',maxlen=max_len)\n",
        "X_test=pad_sequences(X_test, padding='post',maxlen=max_len)"
      ],
      "metadata": {
        "id": "j6mBkHPUnb_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Embedding Matrix\n",
        "### *using pre-trained GloVe Model (Download from GloVe Website)*"
      ],
      "metadata": {
        "id": "qqHhDejs3ApF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "embeddings_dictionary = dict()\n",
        "glove_file = open('a2_glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()"
      ],
      "metadata": {
        "id": "O4NQBU1wncCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = zeros((vocab_length, 100))\n",
        "for word, index in word_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "\n",
        "\n",
        "embedding_matrix.shape"
      ],
      "metadata": {
        "id": "d9sH6KHaqSkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Model Building"
      ],
      "metadata": {
        "id": "XXWu-f1T3Uc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model = Sequential()\n",
        "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=max_len , trainable=False)\n",
        "\n",
        "rnn_model.add(embedding_layer)\n",
        "rnn_model.add(SimpleRNN(128))\n",
        "\n",
        "rnn_model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "7qzHIA1DqTEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model.summary()"
      ],
      "metadata": {
        "id": "_GVhPKRytLQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "vMFADJ2htKqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training RNN Model"
      ],
      "metadata": {
        "id": "-nHavi9e3Y9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model_history = rnn_model.fit(X_train, Y_train, batch_size=128, epochs=10, verbose=1, validation_split=0.2)"
      ],
      "metadata": {
        "id": "rJfVTKYstKs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating RNN Model on test data"
      ],
      "metadata": {
        "id": "Auib9qKY3ciQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_score = rnn_model.evaluate(X_test, Y_test, verbose=1)\n",
        "print(\"Test Accuracy:\", rnn_score[1])"
      ],
      "metadata": {
        "id": "2MbpmONjtKve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(rnn_model_history.history['acc'])\n",
        "plt.plot(rnn_model_history.history['val_acc'])\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(rnn_model_history.history['loss'])\n",
        "plt.plot(rnn_model_history.history['val_loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vheg4_x2uLHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model Building"
      ],
      "metadata": {
        "id": "KToVpzYo3ked"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = Sequential()\n",
        "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=max_len , trainable=False)\n",
        "\n",
        "lstm_model.add(embedding_layer)\n",
        "lstm_model.add(LSTM(128))\n",
        "\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "DSlJBtlyvmqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.summary()"
      ],
      "metadata": {
        "id": "N29FvuVAvwE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training LSTM Model"
      ],
      "metadata": {
        "id": "KWvwh1Z_3nw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "K8g2Ed8rvwHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model_history = lstm_model.fit(X_train, Y_train, batch_size=128, epochs=10, verbose=1, validation_split=0.2)"
      ],
      "metadata": {
        "id": "5F0nvK9VvwJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating LSTM Model on test Data"
      ],
      "metadata": {
        "id": "kg0Sgin83sDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_score = lstm_model.evaluate(X_test, Y_test, verbose=1)\n",
        "print(\"Test Accuracy:\", lstm_score[1])"
      ],
      "metadata": {
        "id": "5q_XbFEtzcKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lstm_model_history.history['acc'])\n",
        "plt.plot(lstm_model_history.history['val_acc'])\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(lstm_model_history.history['loss'])\n",
        "plt.plot(lstm_model_history.history['val_loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aIXm78mKzcOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   RNN Model Gives around 55% accuracy\n",
        "*   LSTM Model gives around 87% accuracy\n",
        "\n",
        "LSTM models outperform simple RNNs due to their ability to capture long-term dependencies through memory cells and gating mechanisms, mitigating the vanishing gradient problem.\n"
      ],
      "metadata": {
        "id": "_YiDVA1x3w4I"
      }
    }
  ]
}